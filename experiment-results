First we considered the default AlertManager and Autoscaler of OpenFaaS, which uses ... But it's very crude, and [cite] has demonstrated that this doesn't work well.

Openfaas also supports using the Kubernets HPA for scaling, so we went with that. Then we ran scaling by the CPU usage resource metric which is what kubernetes supports by default, and that didn't work very well. And it's understandable because [...]
So we understood that the default autoscaler of kubernetes for OpenFaas.

Reasoning for gateway_function_invocation_per_second: We want the number of replicas to scale according to the number of requests coming in to the OpenFaas gateway. Unlike traditional services e.g. Apache web-server which can run serve many requests concurrently, the pods associated with FaaS functions can run only one function invocation at a time. So the only practical way of scaling FaaS pods would be to scale them according to the number of concurrent requests at any given moment. OpenFaaS exposes metrics through which we can get the number of function invocations the OpenFaaS gateway receieves every second, so we went with scaling according to that.


Plots:

The first workload we tried was a gradual linear increase of load which increases the rate of incoming requests per second from 0 to 1 in 4 minutes. 

General Behavior of the Openfaas System under gradual scaling up and then down:

Latency: 

As expected, the requestResponseLatency went up drastically while scaling up. But surprisingly, schedulingLatency didn't flucuate much at all, it was executionLatency which contributed to this fluctuation in the overall latency. 

The only reason executionLatency can go up is because the pods associated with the requests didn't have enough computing resource e.g. CPU and memory. From this, we can gather that OpenFaaS along with Kubernetes is very good at scheduling, but it's resource allocation is where the developer has to pay special attention because that's where most of the inefficiencies come from.

Pod Replicas, Container Per Host, Node CPU Usage, Node Memory Usage:

First of all, as we can see from plot 2, 3, and 4, the control plane uses more resources (cpu and memory) than the other nodes. So it's absolute essential one uses a larger node to use as control plane compared to the smaller worker nodes.

Now when it comes to load balancing between the nodes, it appears that Kubernetes is not very efficient. From the replica plot we can see HPA is very efficient at maintaing the scaling criteria and scaling up the number of pods up and down accordingly. But how the pods do not get uniformly distributed among the nodes. This is also the possible cause behind suboptimal resource allocation to the pods themselves, as overcrowding at a particular node by too many pods would leave little resource to each pods in there.

So, again, resource allocation is something that Kubernetes does not do very well automatically, the developer needs to look into this because this affects the overall performance of the OpenFaaS platform.


Comparison between different workloads::

result-burst-30s-0.1, result-burst-30s-2waves-cold-0.1, result-burst-30s-2waves-hot-0.1

Latency plots:

We can see that the system handles a much faster spike of traffic, i.e. 0 to 1 requests per second in 30 seconds pretty well, the response of the system is almost similar to the gradual scaling up scenario. There is however a slight discrepency in one of the plots, we can see that in the latency plot of result-burst-30s-2waves-cold-0.1 the spike in latency at the scaling up stage is much higher than the other scenarios. This irregular behavior was probably caused because this was the initial test after booting up the system, and that way the system under test might have been different for this and the subsequent tests. One way this difference can occur is that the first time a pod is run on a node, the node pulls the docker image from the registry, which is an expensive task. For subsequent runs of the pod on the same node the docker image already exists in the local filesystem of that node. This might be the reason this irregular behavior of system is being observed, this needs further investigation.


Now comparing the scenarios with 2 waves of requests, i.e. the "cold" one having a longer interval between the 2 waves, and the "hot" one having a shorter interval between the 2 waves, there is a difference in how the system handles the the 2nd wave for these scenarios. For the hot one, the spike in latency for the 2nd wave of scaling up is much shorter compared to the cold one.

This phenomenon can be explained through the concept of cold start and warm starts. For the "hot" scenario, at the time the 2nd wave comes the autoscaler didn't yet scale down, so there were multiple pod replicas running in warm mode, i.e. running but not serving any requests. So when the 2nd wave of request comes the autoscaler doesn't need to create pods to accomodate them, it can use the existing warm pods directly.

Whereas for the cold case, due to there being a larger time interval between the 2 waves the autoscaler scales down to 1 replica by the time the 2nd wave comes. So in order to accomodate that load it needed to create new replicas, and that added to the spike in latency.

From this, we recommend that the developer keeps in mind the expected pattern of request load. If it comes in bursts with shorter interval between the bursts, the HPA cooldown period should be adjusted to accomodate for that, so that it doesn't scale down too quicly and lose out on the opportunity to use warm pods for the subsequent bursts. But if the interval between the bursts is comparatively long, there's no point in not scaling down, because by scaling down we could save CPU and memory usage, which was the purpose of using FaaS in the first place. 


Comparison between different scaling parameters: average function_invocation_per_second of 0.1 vs 0.2:

For the same load, the HPA would create fewer pods for a scaling threshold of 0.2 than it would for 0.1. From the replicas graph we can see that at the peak load the HPA created almost half as many pods for the former than the latter.

But this affects the system adversely. As we can see from the other graphs, the system behaves very irregularly, cpu and memory usage in few of the systems becomes very high and in turn those nodes become unresponsive, the latency of the requests vary wildly, many of the reqeusts fial with 500 code, etc. From this it's clear that OpenFaaS is much more stable when there are larger number of pods, each pod receiving requests slowly, than it is when there are fewer number of pods and requests are queuing up on those pods.

From this, it can be recommended that the developer sets up the scaling parameter in such a way that there are more pods than fewer, so when it comes to the average gateway_function_invocation_per_second metric, the threshold should be on the lower side. The exact amount would depend on the average execution time of the function, for our case 0.1 was a good amount.

A heuristic starting point for calculating the threshold could be the following: (1 / (average execution time of the function)). If Kubernetes was an ideal system which didn't have any other delay and unlimited resources, this should have been enough. But in practice, a value 1/5th of the above value or less works, that way we account for the various inefficiencies of Kubernets and scale pre-emptively.



-----------------------------------

Recommendations:

1. Instead of the Kubernets defaults, use the gateway_function_invocation_per_second as the primary criteria for HPA autoscaling.
2. Pay more attention to resource allocation than scheduling, that's where most of the inefficiencies come from.
3. use a larger node to use as control plane compared to the smaller worker nodes.
4. "Warm up" the nodes before using a system for production, i.e. run various test workloads so the nodes get used to the traffic, e.g. by pulling the required images.
5. we recommend that the developer keeps in mind the expected pattern of request load. If it comes in bursts with shorter interval between the bursts, the HPA cooldown period should be adjusted to accomodate for that, so that it doesn't scale down too quicly and lose out on the opportunity to use warm pods for the subsequent bursts. But if the interval between the bursts is comparatively long, there's no point in not scaling down, because by scaling down we could save CPU and memory usage, which was the purpose of using FaaS in the first place.
6. it can be recommended that the developer sets up the scaling parameter in such a way that there are more pods than fewer, so when it comes to the average gateway_function_invocation_per_second metric, the threshold should be on the lower side. The exact amount would depend on the average execution time of the function.					A heuristic starting point for calculating the threshold could be the following: (1 / (average execution time of the function)). If Kubernetes was an ideal system which didn't have any other delay and unlimited resources, this should have been enough. But in practice, a value 1/5th of the above value or less works, that way we account for the various inefficiencies of Kubernets and scale pre-emptively.
